{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Validation Set Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as skl_lm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll explore the use of the validation set approach in order to estimate the\n",
    "test error rates that result from fitting various linear models on the ${\\tt Auto}$ data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 392 entries, 0 to 391\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           392 non-null    float64\n",
      " 1   cylinders     392 non-null    int64  \n",
      " 2   displacement  392 non-null    float64\n",
      " 3   horsepower    392 non-null    int64  \n",
      " 4   weight        392 non-null    int64  \n",
      " 5   acceleration  392 non-null    float64\n",
      " 6   year          392 non-null    int64  \n",
      " 7   origin        392 non-null    int64  \n",
      " 8   name          392 non-null    object \n",
      "dtypes: float64(3), int64(5), object(1)\n",
      "memory usage: 27.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('datasets/Auto.csv', na_values='?').dropna()\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by using the ${\\tt sample()}$ function to split the set of observations\n",
    "into two halves, by selecting a random subset of 196 observations out of\n",
    "the original 392 observations. We refer to these observations as the training\n",
    "set.\n",
    "\n",
    "We'll use the ${\\tt random\\_state}$ parameter in order to set a seed for\n",
    "${\\tt python}$â€™s random number generator, so that you'll obtain precisely the same results each time. It is generally a good idea to set a random seed when performing an analysis such as cross-validation\n",
    "that contains an element of randomness, so that the results obtained can be reproduced precisely at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df1.sample(196, random_state = 1)\n",
    "test_df = df1[~df1.isin(train_df)].dropna(how = 'all')\n",
    "\n",
    "X_train = train_df['horsepower'].values.reshape(-1,1)\n",
    "y_train = train_df['mpg']\n",
    "X_test = test_df['horsepower'].values.reshape(-1,1)\n",
    "y_test = test_df['mpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use ${\\tt LinearRegression()}$ to fit a linear regression to predict ${\\tt mpg}$ from ${\\tt horsepower}$ using only\n",
    "the observations corresponding to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.5878396082171262, coef: [-0.15964606], intercept: 40.3338030434159\n"
     ]
    }
   ],
   "source": [
    "lm = skl_lm.LinearRegression()\n",
    "#YOUR CODE\n",
    "reg = lm.fit(X_train, y_train)\n",
    "print(f'score: {reg.score(X_test, y_test)}, coef: {reg.coef_}, intercept: {reg.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the ${\\tt predict()}$ function to estimate the response for the test\n",
    "observations, and we use ${\\tt sklearn}$ to caclulate the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.361902892587224"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = lm.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the estimated test MSE for the linear regression fit is 23.36. We\n",
    "can use the ${\\tt PolynomialFeatures()}$ function to estimate the test error for the polynomial\n",
    "and cubic regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_2: 20.252690858346554, mse_3: 20.32560936597247\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2).fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "y_pred_poly = lm.fit(X_train_poly, y_train).predict(poly.transform(X_test))\n",
    "mse_poly_2 = mean_squared_error(y_test, y_pred_poly)\n",
    "\n",
    "poly = PolynomialFeatures(3).fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "y_pred_poly = lm.fit(X_train_poly, y_train).predict(poly.transform(X_test))\n",
    "mse_poly_3 = mean_squared_error(y_test, y_pred_poly)\n",
    "\n",
    "print(f'mse_2: {mse_poly_2}, mse_3: {mse_poly_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These error rates are 20.25 and 20.33, respectively. If we choose a different\n",
    "training set instead, then we will obtain somewhat different errors on the\n",
    "validation set. We can test this out by setting a different random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 25.10853905288967, mse_2: 19.722533470492426, mse_3: 19.92136786007268\n"
     ]
    }
   ],
   "source": [
    "train_df = df1.sample(196, random_state = 2)\n",
    "test_df = df1[~df1.isin(train_df)].dropna(how = 'all')\n",
    "\n",
    "X_train = train_df['horsepower'].values.reshape(-1,1)\n",
    "y_train = train_df['mpg']\n",
    "X_test = test_df['horsepower'].values.reshape(-1,1)\n",
    "y_test = test_df['mpg']\n",
    "\n",
    "#YOUR CODE\n",
    "reg = lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "poly = PolynomialFeatures(2).fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "y_pred_poly = lm.fit(X_train_poly, y_train).predict(poly.transform(X_test))\n",
    "mse_poly_2 = mean_squared_error(y_test, y_pred_poly)\n",
    "\n",
    "poly = PolynomialFeatures(3).fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "y_pred_poly = lm.fit(X_train_poly, y_train).predict(poly.transform(X_test))\n",
    "mse_poly_3 = mean_squared_error(y_test, y_pred_poly)\n",
    "\n",
    "print(f'mse: {mse}, mse_2: {mse_poly_2}, mse_3: {mse_poly_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this split of the observations into a training set and a validation\n",
    "set, we find that the validation set error rates for the models with linear,\n",
    "quadratic, and cubic terms are 25.11, 19.72, and 19.92, respectively.\n",
    "\n",
    "These results are consistent with our previous findings: a model that\n",
    "predicts ${\\tt mpg}$ using a quadratic function of ${\\tt horsepower}$ performs better than\n",
    "a model that involves only a linear function of ${\\tt horsepower}$, and there is\n",
    "little evidence in favor of a model that uses a cubic function of ${\\tt horsepower}$.\n",
    "\n",
    "# Leave-One-Out Cross-Validation\n",
    "\n",
    "The LOOCV estimate can be automatically computed for any generalized linear model using the `LeaveOneOut()` and `KFold()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 24.231513517929226, std: 36.79731503640535\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE\n",
    "# determine MSE and STD for a given number of folds (.eg 392)\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=392, shuffle=True, random_state=1)\n",
    "mse_values = []\n",
    "for train_index, test_index in kf.split(df1):\n",
    "    X_train = df1.iloc[train_index]['horsepower'].values.reshape(-1,1)\n",
    "    y_train = df1.iloc[train_index]['mpg']\n",
    "    X_test = df1.iloc[test_index]['horsepower'].values.reshape(-1,1)\n",
    "    y_test = df1.iloc[test_index]['mpg']\n",
    "    reg = lm.fit(X_train, y_train)\n",
    "    y_pred = lm.predict(X_test)\n",
    "    mse_values.append(mean_squared_error(y_test, y_pred))\n",
    "mse_mean = np.mean(mse_values)\n",
    "mse_std = np.std(mse_values) \n",
    "print(f'mse: {mse_mean}, std: {mse_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cross-validation estimate for the test error is approximately 24.23. We can repeat this procedure for increasingly complex polynomial fits.\n",
    "To automate the process, we use the `for()` function to initiate a for loop\n",
    "which iteratively fits polynomial regressions for polynomials of order `i = 1`\n",
    "to `i = 5` and computes the associated cross-validation error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly: 1 - mse: 24.23151351792922, std: 36.79731503640535\n",
      "poly: 2 - mse: 19.24821312448968, std: 34.998446151782325\n",
      "poly: 3 - mse: 19.33498406412144, std: 35.765135677973824\n",
      "poly: 4 - mse: 19.424430308539886, std: 35.683352757212276\n",
      "poly: 5 - mse: 19.033216707145417, std: 35.31733192537356\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE\n",
    "# determine MSE and STD for polynomial of different degrees\n",
    "for i in range(1, 6):\n",
    "    mse_values = []\n",
    "    for train_index, test_index in kf.split(df1):\n",
    "        X_train = df1.iloc[train_index]['horsepower'].values.reshape(-1,1)\n",
    "        y_train = df1.iloc[train_index]['mpg']\n",
    "        X_test = df1.iloc[test_index]['horsepower'].values.reshape(-1,1)\n",
    "        y_test = df1.iloc[test_index]['mpg']\n",
    "        poly = PolynomialFeatures(i).fit(X_train)\n",
    "        X_train_poly = poly.transform(X_train)\n",
    "        y_pred_poly = lm.fit(X_train_poly, y_train).predict(poly.transform(X_test))\n",
    "        mse_values.append(mean_squared_error(y_test, y_pred_poly))\n",
    "    mse_mean = np.mean(mse_values)\n",
    "    mse_std = np.std(mse_values) \n",
    "    print(f'poly: {i} - mse: {mse_mean}, std: {mse_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a sharp drop in the estimated test MSE between\n",
    "the linear and quadratic fits, but then no clear improvement from using\n",
    "higher-order polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Fold Cross-Validation\n",
    "\n",
    "The `KFold` function can (intuitively) also be used to implement `k`-fold CV. Below we\n",
    "use `k = 10`, a common choice for `k`, on the `Auto` data set. We once again set\n",
    "a random seed and initialize a vector in which we will print the CV errors\n",
    "corresponding to the polynomial fits of orders one to ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly: 1 - mse: 24.21965632497301, std: 5.655569942971911\n",
      "poly: 2 - mse: 19.34905165299215, std: 6.75998179990019\n",
      "poly: 3 - mse: 19.416177107084287, std: 6.847547439455138\n",
      "poly: 4 - mse: 19.440516673950192, std: 6.793107079879531\n",
      "poly: 5 - mse: 18.981042354511565, std: 6.7913104674803115\n",
      "poly: 6 - mse: 18.82446367095536, std: 6.759783670485956\n",
      "poly: 7 - mse: 19.046671259975064, std: 6.661939491567656\n",
      "poly: 8 - mse: 19.223943478459454, std: 6.676990786105731\n",
      "poly: 9 - mse: 19.200980451554507, std: 6.778080978380523\n",
      "poly: 10 - mse: 19.020593973979583, std: 6.798737088198012\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE\n",
    "# determine MSE and STD for polynomial of different degrees\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv = KFold(10, shuffle=True, random_state=1)\n",
    "for i in range(1, 11):\n",
    "    poly = PolynomialFeatures(i).fit(X_train)\n",
    "    X_train_poly = poly.transform(X_train)\n",
    "    scores = cross_val_score(lm, X_train_poly, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "    mse_mean = np.mean(-scores)\n",
    "    mse_std = np.std(-scores) \n",
    "    print(f'poly: {i} - mse: {mse_mean}, std: {mse_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
